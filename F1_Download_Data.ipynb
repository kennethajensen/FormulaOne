{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1wtmo7PnfLJowm-lqeekX672HnR1e3JD-",
      "authorship_tag": "ABX9TyMQPVX6ybuCfjA5KJnMjgrl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kennethajensen/FormulaOne/blob/main/F1_Download_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TKAvRAsnMYA"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import empty\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Set the base URL for the OpenF1 API\n",
        "base_url = 'https://api.openf1.org/v1/'\n",
        "\n",
        "# Mount my Google Drive and set the path for the data storage\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/Data Science/[02] Articles - Formula 1 [Work-in-progress]/Data/'\n",
        "\n",
        "# Names for data files stored locally\n",
        "meetings_file_name     = 'meetings.csv'\n",
        "sessions_file_name     = 'qualifying_sessions.csv'\n",
        "laps_file_name         = 'qualifying_laps.csv'\n",
        "fastest_laps_file_name = 'qualifying_fastest_laps.csv'\n",
        "car_data_file_name     = 'car_data.csv'\n",
        "location_file_name     = 'location.csv'\n",
        "# Full file paths\n",
        "meetings_path     = os.path.join(data_path, meetings_file_name)\n",
        "sessions_path     = os.path.join(data_path, sessions_file_name)\n",
        "laps_path         = os.path.join(data_path, laps_file_name)\n",
        "fastest_laps_path = os.path.join(data_path, fastest_laps_file_name)\n",
        "car_data_path     = os.path.join(data_path, car_data_file_name)\n",
        "location_path     = os.path.join(data_path, location_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(endpoint, filter='', max_retries=5, initial_retry_delay=2):\n",
        "    \"\"\"\n",
        "    Retrieves data from an API endpoint.\n",
        "    Optionally, applies a filter to select the record to include.\n",
        "\n",
        "    Args:\n",
        "        endpoint (str): The endpoint name to query.\n",
        "        filter (str): The data filter to apply to the request\n",
        "        max_retries (int): The maximum number of retries to attempt when encountering\n",
        "                           a 429 (Too many requests) error.\n",
        "        initial_retry_delay (int): The initial delay between retries\n",
        "    \"\"\"\n",
        "\n",
        "    retries = 0\n",
        "    retry_delay = initial_retry_delay\n",
        "    while retries <= max_retries:\n",
        "      try:\n",
        "        if filter:\n",
        "          response = urlopen(base_url + endpoint + '?' + filter)\n",
        "        else:\n",
        "          response = urlopen(base_url + endpoint)\n",
        "        data = json.loads(response.read().decode('utf-8'))\n",
        "        df = pd.DataFrame(data)\n",
        "        time.sleep(1)   # Pause for 1 second to avoid rate limiting\n",
        "        return df\n",
        "      except HTTPError as e:\n",
        "        if e.code == 429:   # Too Many Requests\n",
        "          print(f\"Rate limit hit for {endpoint}?{filter}. Retrying in {retry_delay} seconds (Retry {retries+1}/{max_retries})...\")\n",
        "          time.sleep(retry_delay)\n",
        "          retries += 1\n",
        "          retry_delay *= 2   # Exponential backoff\n",
        "        else:\n",
        "          raise   # Re-raise other HTTP errors immediately\n",
        "      except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        raise   # Re-raise other unexpected errors\n",
        "\n",
        "    raise Exception(f\"Failed to retrieve data for {endpoint}?{filter} after {max_retries} retries.\")"
      ],
      "metadata": {
        "id": "mqlOjY7AtALt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a753f55b"
      },
      "source": [
        "def save_dataframe_to_csv(df, path):\n",
        "    \"\"\"\n",
        "    Saves a DataFrame to a CSV file. If the file exists, it appends the data without headers.\n",
        "    Otherwise, it creates a new file with headers.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to save.\n",
        "        path (str): The file path where the CSV should be saved.\n",
        "    \"\"\"\n",
        "    if os.path.exists(path):\n",
        "        df.to_csv(path, mode='a', header=False, index=False)\n",
        "        print(f\"Appended {len(df)} rows to existing file: {path}\")\n",
        "    else:\n",
        "        df.to_csv(path, mode='w', header=True, index=False)\n",
        "        print(f\"Created new file and wrote {len(df)} rows: {path}\")\n",
        "\n",
        "print(\"Defined save_dataframe_to_csv_smart function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meetings\n",
        "First get the complete data set with all **Meetings** where each meeting is either a testing or racing event covering multiple sessions and days.\\\n",
        "Save the data to a CSV file replacing any previous file."
      ],
      "metadata": {
        "id": "99LFxEbtqsK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meetings = get_data('meetings')\n",
        "meetings.to_csv(meetings_path, index=False)"
      ],
      "metadata": {
        "id": "_H1ppOuCtVR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qualifying sessions\n",
        "Second, get all of the qualifying sessions. This includes both the sprint qualifying and the qualifying for the feature race.\\\n",
        "Save the data to a CSV file replacing any previous file.\n",
        "\n"
      ],
      "metadata": {
        "id": "LrHNzZs5Lg2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sessions_filter = 'session_type=Qualifying \\\n",
        "                   &year>=2023 \\\n",
        "                   &country_code=USA'\n",
        "sessions_filter = sessions_filter.replace(' ', '')   # Remove whitespace\n",
        "\n",
        "sessions = get_data(endpoint = 'sessions',\n",
        "                    filter = sessions_filter)\n",
        "\n",
        "# Convert the date columns to datetime objects for comparison\n",
        "sessions['date_start'] = pd.to_datetime(sessions['date_start'])\n",
        "sessions['date_end'] = pd.to_datetime(sessions['date_end'])\n",
        "# Remove all session where the end time is after the current time\n",
        "sessions = sessions[sessions['date_end'] <= pd.Timestamp.now(tz='UTC')]\n",
        "\n",
        "sessions.to_csv(sessions_path, index=False)\n",
        "\n",
        "# Create a list of all the unique sessions\n",
        "# The session_key should be unique in the dataframe to begin with\n",
        "all_session_keys = sessions['session_key'].unique()\n",
        "print(f\"Total number of sessions: {len(all_session_keys)}\")"
      ],
      "metadata": {
        "id": "kw9jUxW7t65C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laps\n",
        "Retrieve the Lap data and store it in a local file before identifying the fastest qualifying laps for each driver in each session.\n",
        "- First check for an existing CSV file and the laps that are already stored in the file.\n",
        "- Get the lap data from the API but only from any qualifying sessions that have not been retrieved earlier.\n",
        "- Then append the newly retrieved data to the existing CSV file.\n",
        "\n"
      ],
      "metadata": {
        "id": "WoQVou1w0sqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "laps_csv_file_exists = os.path.exists(laps_path)\n",
        "\n",
        "if not laps_csv_file_exists:\n",
        "    # Create an empty dataframe\n",
        "    laps_from_csv = []\n",
        "    retrieved_session_keys = empty(0)\n",
        "else:\n",
        "    # Load the data from the file\n",
        "    laps_from_csv = pd.read_csv(laps_path)\n",
        "    # Get a list of unique sessions\n",
        "    retrieved_session_keys = laps_from_csv['session_key'].unique()\n",
        "\n",
        "# Identify the sessions that do not appear in the 'Laps' data file\n",
        "# The laps from missing sessions will need to be retrived\n",
        "unretrieved_session_keys = list(set(all_session_keys) - set(retrieved_session_keys))\n",
        "\n",
        "print(f\"Number of sessions already retrieved: {len(retrieved_session_keys)}\")\n",
        "print(f\"Number of sessions to be retrieved: {len(unretrieved_session_keys)}\")\n",
        "# The lap data from the qualifying in Baku in 2025 is missing from OpenF1.org\n",
        "# and the program will try to pick it up every time it is executed"
      ],
      "metadata": {
        "id": "wV57ac6VL9Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a7c5fc6"
      },
      "source": [
        "laps = []\n",
        "\n",
        "# Get the laps from every session not already retrieved\n",
        "for session_key in unretrieved_session_keys:\n",
        "    laps_by_session = get_data('laps', f'session_key={session_key}')\n",
        "    laps.append(laps_by_session)\n",
        "\n",
        "print(f\"Fetched lap data for {len(laps)} sessions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the lap data to a file\n",
        "* Append the newly retrieved laps to the existing `CSV` file or create the file if the file does not already exist.\n",
        "* If `laps` is currently a list of DataFrames, we will concatenate it into a single DataFrame.\n"
      ],
      "metadata": {
        "id": "Pn_4V-kSo8UC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43384980"
      },
      "source": [
        "if laps:\n",
        "    # The laps dataframe is a dataframe of dataframes\n",
        "    # This collapses the dataframes\n",
        "    laps = pd.concat(laps, ignore_index=True)\n",
        "    save_dataframe_to_csv(laps, laps_path)\n",
        "\n",
        "else:\n",
        "    print(\"No new lap data to append or save.\")\n",
        "    laps = pd.DataFrame() # Ensure laps is always a DataFrame, even if empty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46366ade"
      },
      "source": [
        "## Identify the fastest lap from each driver in every session\n",
        "\n",
        "To find the fastest lap for each driver in each session, we need to:\n",
        "1. Ensure the `laps` DataFrame contains data.\\\n",
        " If `laps` is currently a list of DataFrames, we will concatenate it into a single DataFrame.\n",
        "2. Group the DataFrame by `session_key` and `driver_number`.\n",
        "3. For each group, find the row with the minimum `lap_duration`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5da20df3"
      },
      "source": [
        "if not laps.empty:\n",
        "    # Create a copy to avoid SettingWithCopyWarning and resets the index\n",
        "    fastest_laps = laps.copy().reset_index(drop=True)\n",
        "\n",
        "    # Convert is_pit_out_lap to boolean and drop rows where is_pit_out_lap is True\n",
        "    fastest_laps['is_pit_out_lap'] = fastest_laps['is_pit_out_lap'].astype(bool)\n",
        "    fastest_laps = fastest_laps[fastest_laps['is_pit_out_lap'] == False]\n",
        "    # Ensure 'date_start' is datetime and 'lap_duration' is numeric\n",
        "    fastest_laps['date_start'] = pd.to_datetime(fastest_laps['date_start'],\n",
        "                                                format='ISO8601')\n",
        "    fastest_laps['lap_duration'] = pd.to_numeric(fastest_laps['lap_duration'],\n",
        "                                                 errors='coerce')\n",
        "    # Drop rows where lap_duration is NaN (couldn't be converted)\n",
        "    fastest_laps.dropna(subset=['lap_duration'], inplace=True)\n",
        "\n",
        "    # Calculate 'date_end' by adding 'lap_duration' to 'date_start'\n",
        "    fastest_laps['date_end'] = fastest_laps['date_start'] \\\n",
        "                               + pd.to_timedelta(fastest_laps['lap_duration'], unit='s')\n",
        "\n",
        "    # Find the fastest lap for each driver in each session\n",
        "    fastest_laps = fastest_laps.loc[fastest_laps.groupby(['session_key', 'driver_number'])['lap_duration'].idxmin()]\n",
        "\n",
        "    print(f\"Found {len(fastest_laps)} fastest laps.\")\n",
        "\n",
        "    save_dataframe_to_csv(fastest_laps, fastest_laps_path)\n",
        "\n",
        "else:\n",
        "    fastest_laps = pd.DataFrame() # Ensure fastest_laps is always a DataFrame, even if empty\n",
        "    print(\"No lap data available to process.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get a complete list of all qualifying laps\n",
        "\n",
        "\n",
        "*   Combine the data from the CSV file with any newly retrieved qualifying laps\n",
        "*   Get evey unique combination of the session and the driver\n",
        "* Merge in the start and end time for each of the fastest qualifying laps\n",
        "\n",
        "This will be used to get the car and location data for just those laps.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xf69_Zcr2089"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the laps from the CSV file with the newly retrieved laps\n",
        "# to get a complete list\n",
        "all_qualifying_laps = pd.concat([laps_from_csv, laps], ignore_index=True)\n",
        "\n",
        "\n",
        "unique_laps = all_qualifying_laps[['session_key','driver_number']].drop_duplicates()\n",
        "\n",
        "display(unique_laps)"
      ],
      "metadata": {
        "id": "IKvGz1UOsZ86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(unique_laps)"
      ],
      "metadata": {
        "id": "33z3JIydDVUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f35bb42e"
      },
      "source": [
        "if not unique_laps.empty and not fastest_laps.empty:\n",
        "    # Select only the necessary columns from fastest_laps to merge\n",
        "    fastest_laps_for_merge = fastest_laps[['session_key', 'driver_number', 'date_start', 'date_end']]\n",
        "\n",
        "    # Merge unique_laps_keys with these selected columns\n",
        "    unique_laps = pd.merge(\n",
        "        unique_laps,\n",
        "        fastest_laps_for_merge,\n",
        "        on=['session_key', 'driver_number'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    print(\"Added 'date_start' and 'date_end' to unique_laps_keys.\")\n",
        "    display(unique_laps.head())\n",
        "else:\n",
        "    print(\"Either unique_laps or fastest_laps DataFrame is empty, cannot add dates.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Car data"
      ],
      "metadata": {
        "id": "Bae7Hp0D38Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car_data_csv_file_exists = os.path.exists(car_data_path)\n",
        "\n",
        "if not car_data_csv_file_exists:\n",
        "    # Create an empty dataframe with the expected columns\n",
        "    car_data_from_csv = pd.DataFrame(columns=['session_key', 'driver_number'])\n",
        "    retrieved_car_data_laps = pd.DataFrame(columns=['session_key', 'driver_number'])\n",
        "else:\n",
        "    # Load the data from the file\n",
        "    car_data_from_csv = pd.read_csv(car_data_path)\n",
        "    # Get a list of unique session_key and driver_number combinations that have car data\n",
        "    retrieved_car_data_laps = car_data_from_csv[['session_key','driver_number']].drop_duplicates()\n",
        "\n",
        "# Convert DataFrames to sets of tuples for efficient comparison\n",
        "# This creates a unique identifier for each session-driver combination (session_key, driver_number)\n",
        "set_unique_laps = set(tuple(row) for row in unique_laps[['session_key', 'driver_number']].values)\n",
        "set_retrieved_car_data_laps = set(tuple(row) for row in retrieved_car_data_laps.values)\n",
        "\n",
        "# Identify the unique session_key and driver_number combinations that have not been retrieved\n",
        "unretrieved_combinations_set = set_unique_laps - set_retrieved_car_data_laps\n",
        "\n",
        "# Convert the set of unretrieved combinations back into a DataFrame\n",
        "unretrieved_car_data_laps = pd.DataFrame(list(unretrieved_combinations_set), columns=['session_key', 'driver_number'])\n",
        "\n",
        "\n",
        "if not unretrieved_car_data_laps.empty and not fastest_laps.empty:\n",
        "    # Now, merge with the unique_laps DataFrame to get the date_start and\n",
        "    # date_end for these unretrieved combinations\n",
        "    unretrieved_car_data_laps = pd.merge(\n",
        "        unretrieved_car_data_laps,\n",
        "        unique_laps[['session_key', 'driver_number', 'date_start', 'date_end']],\n",
        "        on=['session_key', 'driver_number'],\n",
        "        how='left'\n",
        "    )\n",
        "    # Remove records that do not have both a date_start and a date_end\n",
        "    unretrieved_car_data_laps.dropna(subset=['date_start', 'date_end'], inplace=True)\n",
        "else:\n",
        "    unretrieved_car_data_laps = pd.DataFrame(columns=['session_key', 'driver_number', 'date_start', 'date_end'])\n",
        "\n",
        "print(f\"Number of laps already retrieved: {len(retrieved_car_data_laps)}\")\n",
        "print(f\"Number of laps to be retrieved: {len(unretrieved_car_data_laps)}\")"
      ],
      "metadata": {
        "id": "E3TiKIjcrLnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98506594"
      },
      "source": [
        "car_data = []\n",
        "\n",
        "# Iterate through each of the missing laps\n",
        "for index, row in unretrieved_car_data_laps.iterrows():\n",
        "\n",
        "    # Get the session and driver number from the current row\n",
        "    session_key = row['session_key']\n",
        "    driver_number = row['driver_number']\n",
        "\n",
        "    # date_start and date_end columns are already datetime objects from previous steps\n",
        "    # We need to ensure they are UTC and then format them.\n",
        "    date_start_utc = row['date_start'].tz_convert('UTC')\n",
        "    date_end_utc = row['date_end'].tz_convert('UTC')\n",
        "\n",
        "    formatted_date_start = date_start_utc.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "    formatted_date_end = date_end_utc.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "    data_filter = f\"session_key={session_key}& \\\n",
        "                    driver_number={driver_number}& \\\n",
        "                    date>={formatted_date_start}& \\\n",
        "                    date<={formatted_date_end}\"\n",
        "    data_filter = data_filter.replace(' ', '')\n",
        "\n",
        "    # Fetching 'car_data'\n",
        "    car_data_by_session_driver = get_data('car_data', data_filter)\n",
        "    if not car_data_by_session_driver.empty:\n",
        "        car_data.append(car_data_by_session_driver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if car_data:\n",
        "    # The 'car_data' dataframe is a dataframe of dataframes\n",
        "    # This collapses the dataframes\n",
        "    car_data = pd.concat(car_data, ignore_index=True)\n",
        "    save_dataframe_to_csv(df=car_data, path=car_data_path)\n",
        "\n",
        "else:\n",
        "    print(\"No new car data to append or save.\")"
      ],
      "metadata": {
        "id": "mUjvlH7YixcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Location"
      ],
      "metadata": {
        "id": "WdIcrcaw5QjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "location_csv_file_exists = os.path.exists(car_data_path)\n",
        "\n",
        "if not location_csv_file_exists:\n",
        "    # Create an empty dataframe with the expected columns\n",
        "    location_from_csv = pd.DataFrame(columns=['session_key', 'driver_number'])\n",
        "    retrieved_location_laps = pd.DataFrame(columns=['session_key', 'driver_number'])\n",
        "else:\n",
        "    # Load the data from the file\n",
        "    location_from_csv = pd.read_csv(location_path)\n",
        "    # Get a list of unique session_key and driver_number combinations that have car data\n",
        "    retrieved_location_laps = location_from_csv[['session_key','driver_number']].drop_duplicates()\n",
        "\n",
        "# Convert DataFrames to sets of tuples for efficient comparison\n",
        "# This creates a unique identifier for each session-driver combination (session_key, driver_number)\n",
        "set_unique_laps = set(tuple(row) for row in unique_laps[['session_key', 'driver_number']].values)\n",
        "set_retrieved_location_laps = set(tuple(row) for row in retrieved_location_laps.values)\n",
        "\n",
        "# Identify the unique session_key and driver_number combinations that have not been retrieved\n",
        "unretrieved_combinations_set = set_unique_laps - set_retrieved_location_laps\n",
        "\n",
        "# Convert the set of unretrieved combinations back into a DataFrame\n",
        "unretrieved_location_laps = pd.DataFrame(list(unretrieved_combinations_set), columns=['session_key', 'driver_number'])\n",
        "\n",
        "if not unretrieved_location_laps.empty and not fastest_laps.empty:\n",
        "    # Now, merge with the unique_laps DataFrame to get the date_start and\n",
        "    # date_end for these unretrieved combinations\n",
        "    unretrieved_location_laps = pd.merge(\n",
        "        unretrieved_location_laps,\n",
        "        unique_laps[['session_key', 'driver_number', 'date_start', 'date_end']],\n",
        "        on=['session_key', 'driver_number'],\n",
        "        how='left'\n",
        "    )\n",
        "    # Remove records that do not have both a date_start and a date_end\n",
        "    unretrieved_location_laps.dropna(subset=['date_start', 'date_end'], inplace=True)\n",
        "else:\n",
        "    unretrieved_location_laps = pd.DataFrame(columns=['session_key', 'driver_number', 'date_start', 'date_end'])\n",
        "\n",
        "print(f\"Number of laps already retrieved: {len(retrieved_location_laps)}\")\n",
        "print(f\"Number of laps to be retrieved: {len(unretrieved_location_laps)}\")"
      ],
      "metadata": {
        "id": "DNdrUqzD47Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN9xiL8C49Ks"
      },
      "source": [
        "location = []\n",
        "\n",
        "# Iterate through each of the missing laps\n",
        "for index, row in unretrieved_location_laps.iterrows():\n",
        "    # Get the session and driver number from the current row\n",
        "    session_key = row['session_key']\n",
        "    driver_number = row['driver_number']\n",
        "\n",
        "    # date_start and date_end columns are already datetime objects from previous steps\n",
        "    # We need to ensure they are UTC and then format them.\n",
        "    date_start_utc = row['date_start'].tz_convert('UTC')\n",
        "    date_end_utc = row['date_end'].tz_convert('UTC')\n",
        "\n",
        "    formatted_date_start = date_start_utc.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "    formatted_date_end = date_end_utc.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "    data_filter = f\"session_key={session_key}& \\\n",
        "                    driver_number={driver_number}& \\\n",
        "                    date>={formatted_date_start}& \\\n",
        "                    date<={formatted_date_end}\"\n",
        "    data_filter = data_filter.replace(' ', '')\n",
        "\n",
        "    # Fetching 'location'\n",
        "    location_by_session_driver = get_data('location', data_filter)\n",
        "    if not location_by_session_driver.empty:\n",
        "        location.append(location_by_session_driver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if location:\n",
        "    # The 'location' dataframe is a dataframe of dataframes\n",
        "    # This collapses the dataframes\n",
        "    location = pd.concat(location, ignore_index=True)\n",
        "    save_dataframe_to_csv(df=location, path=location_path)\n",
        "\n",
        "else:\n",
        "    print(\"No new location data to append or save.\")"
      ],
      "metadata": {
        "id": "8wVO-EpVj6Da"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}