{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1wtmo7PnfLJowm-lqeekX672HnR1e3JD-",
      "authorship_tag": "ABX9TyPELWaQcg19fXG39atB+7F5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kennethajensen/FormulaOne/blob/main/F1_Download_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "178c6da0"
      },
      "source": [
        "# **Analyzing Formula 1 Data (Part 1)**\n",
        "## *How do I get some data, anyway?*\n",
        "\n",
        "---\n",
        "\n",
        "This notebook retrieves and processes Formula 1 qualifying session data from [OpenF1](https://openf1.org/)'s open-source API.\n",
        "\n",
        "The goal is to use this data to create a profile of how each track is driven and to then analyze the results to find groups of tracks that are similar and identify any anomalies or changes over time.\n",
        "\n",
        "The article [Analyzing Formula 1 Data | How do I get some data, anyway?](https://medium.com/@kenneth.agregaard.jensen/analyzing-formula-1-data-part-1-5b745527516a) describes the project and the data retrieval in more detail and also contains links to the further analysis and results.\n",
        "\n",
        "This notebook collects the source data needed to perform the analyses that I currently have in mind. All data is saved locally as CSV files for persistent storage to avoid having to request the same data repeatedly from the API.\n",
        "\n",
        "**Output:**\\\n",
        "A series of locally stored CSV files with the relevant data from the OpenF1 APIs.\n",
        "- **Meetings:**\\\n",
        "  A record for each event on the Formula 1 calendar including pre-season testing events and regular race weekends.\n",
        "- **Qualifying Sessions:** \\\n",
        "  A record for each qualifying session held during every meeting. These include both sprint qualifying and qualifying for the feature race.\n",
        "- **Drivers:**\\\n",
        "  A record for each driver in every qualifying sessions held during every meeting. This contains the driver's number, name, team, nationality, and so on.\n",
        "- **Qualifying Laps:** \\\n",
        "  One record for each lap during every qualifying session.\n",
        "- **Fastest Qualifying Laps:** \\\n",
        "  The fastest qualifying lap from each driver in each qualifying session.\n",
        "- **Car Data:** \\\n",
        "  The car's telemetry including speed, selected gear, throttle position, and brake usage during all fastest qualifying laps.\\\n",
        "  Averages around 4 records per second.\n",
        "- **Location:** \\\n",
        "  The car's location in three dimensions during all fastest qualifying laps.\\\n",
        "  Averages around 4 records per second.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TKAvRAsnMYA"
      },
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "\n",
        "# Used when querying the API endpoints\n",
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "# Used to convert the JSON response from the API to data tables\n",
        "import json\n",
        "# Allows access to Google Drive\n",
        "from google.colab import drive\n",
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import empty\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set the base URL for the OpenF1 API\n",
        "# Used by the 'get_data' function\n",
        "base_url = 'https://api.openf1.org/v1/'\n",
        "\n",
        "# Mount my Google Drive and set the path for the data storage\n",
        "# Used when reading from and writing to Google Drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/Data Science/[02] Articles - Formula 1 [Work-in-progress]/Data/'\n",
        "\n",
        "# Names for data files stored on Google Drive\n",
        "meetings_file_name     = 'meetings.csv'\n",
        "sessions_file_name     = 'qualifying_sessions.csv'\n",
        "drivers_file_name      = 'drivers.csv'\n",
        "laps_file_name         = 'qualifying_laps.csv'\n",
        "fastest_laps_file_name = 'qualifying_fastest_laps.csv'\n",
        "car_data_file_name     = 'car_data.csv'\n",
        "location_file_name     = 'location.csv'\n",
        "# Full file paths\n",
        "meetings_path     = os.path.join(data_path, meetings_file_name)\n",
        "sessions_path     = os.path.join(data_path, sessions_file_name)\n",
        "drivers_path      = os.path.join(data_path, drivers_file_name)\n",
        "laps_path         = os.path.join(data_path, laps_file_name)\n",
        "fastest_laps_path = os.path.join(data_path, fastest_laps_file_name)\n",
        "car_data_path     = os.path.join(data_path, car_data_file_name)\n",
        "location_path     = os.path.join(data_path, location_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function: `get_data`\n",
        "\n",
        "The `get_data` function is a crucial utility in this notebook for safely retrieving data from the OpenF1 API.\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "It's designed to fetch data from a specified API endpoint, optionally applying a filter. It also includes robust error handling, especially for API rate limits.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "- `endpoint` (str): The specific API resource you want to query (e.g., `meetings`, `laps`).\n",
        "- `filter` (str): An optional string containing URL query parameters to refine your request (e.g., `session_type=Qualifying`).\\\n",
        "It defaults to an empty string if no filter is provided.\n",
        "- `max_retries` (int): The maximum number of times the function will attempt to retry the API call if a rate limit (HTTP 429) error occurs. Defaults to 5.\n",
        "- `initial_retry_delay` (int): The starting delay in seconds before the first retry. Defaults to 2.\n",
        "\n",
        "**Functionality:**\n",
        "\n",
        "1. **Retry Loop:** It uses a `while` loop to attempt the API request up to `max_retries` times.\n",
        "2. **API Request:** It constructs the full API URL using the `base_url`, `endpoint`, and `filter`, then makes the request using `urlopen()`.\n",
        "3. **JSON Parsing:** The response is read, decoded from bytes to UTF-8, and then parsed as a JSON object into a Python list of dictionaries.\n",
        "4. **DataFrame Conversion:** This JSON data is then converted into a Pandas DataFrame, making it easy to work with.\n",
        "5. Rate Limit Handling (HTTP 429): If the API returns an HTTP 429 (Too Many Requests) error, the function prints a message, pauses for `retry_delay` seconds (which doubles with each subsequent retry â€“ an exponential backoff strategy), and then retries the request.\n",
        "6. **Other Error Handling:** Any other HTTP errors or unexpected exceptions are immediately re-raised, as they indicate a different kind of issue.\n",
        "7. **Success:** If the request is successful, the DataFrame is returned.\n",
        "8. **Failure:** If all retries fail, an exception is raised, indicating that the data could not be retrieved.\n",
        "9. **Delay:** After a successful API call, it pauses for 1 second (`time.sleep(1)`) to proactively avoid hitting rate limits on subsequent calls."
      ],
      "metadata": {
        "id": "qsZBa0l7Ni0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(endpoint, filter='', max_retries=5, initial_retry_delay=2):\n",
        "    \"\"\"\n",
        "    Retrieves data from an API endpoint.\n",
        "    Optionally, applies a filter to select the record to include.\n",
        "\n",
        "    Args:\n",
        "        endpoint (str): The endpoint name to query.\n",
        "        filter (str): The data filter to apply to the request\n",
        "        max_retries (int): The maximum number of retries to attempt when encountering\n",
        "                           a 429 (Too many requests) error.\n",
        "        initial_retry_delay (int): The initial delay between retries\n",
        "    \"\"\"\n",
        "\n",
        "    retries = 0\n",
        "    retry_delay = initial_retry_delay\n",
        "    while retries <= max_retries:\n",
        "      try:\n",
        "        if filter:\n",
        "          response = urlopen(base_url + endpoint + '?' + filter)\n",
        "        else:\n",
        "          response = urlopen(base_url + endpoint)\n",
        "        data = json.loads(response.read().decode('utf-8'))\n",
        "        df = pd.DataFrame(data)\n",
        "        time.sleep(1)   # Pause for 1 second to avoid rate limiting\n",
        "        return df\n",
        "      except HTTPError as e:\n",
        "        if e.code == 429:   # Too Many Requests\n",
        "          print(f\"Rate limit hit for {endpoint}?{filter}. Retrying in {retry_delay} seconds (Retry {retries+1}/{max_retries})...\")\n",
        "          time.sleep(retry_delay)\n",
        "          retries += 1\n",
        "          retry_delay *= 2   # Exponential backoff\n",
        "        else:\n",
        "          raise   # Re-raise other HTTP errors immediately\n",
        "      except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        raise   # Re-raise other unexpected errors\n",
        "\n",
        "    raise Exception(f\"Failed to retrieve data for {endpoint}?{filter} after {max_retries} retries.\")"
      ],
      "metadata": {
        "id": "mqlOjY7AtALt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function: `save_dataframe_to_csv`\n",
        "The `save_dataframe_to_csv` function is a utility designed to save a Pandas DataFrame to a CSV file in a smart way.\n",
        "\n",
        "**Purpose:** It saves a DataFrame to a specified file path. Its key feature is that it can either create a new file (including headers) or append data to an existing file (without adding duplicate headers).\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "- `df` (pd.DataFrame): This is the Pandas DataFrame that you want to save.\n",
        "- `path` (str): This is the full file path (including the file name and extension, e.g., `/content/drive/MyDrive/data.csv`) where the CSV should be saved.\n",
        "\n",
        "**Functionality:**\n",
        "\n",
        "1. **Check for Existing File:** It first checks if a file already exists at the given `path` using `os.path.exists(path)`.\n",
        "2. **Append Data:** If the file does exist, it appends the DataFrame's data to the end of that file. It uses `mode='a'` (append mode) and `header=False` (to avoid writing headers again) in the `df.to_csv()` call.\n",
        "3. **Create New File:** If the file does not exist, it creates a new CSV file at the specified path. It uses `mode='w'` (write mode, which creates a new file or overwrites an existing one) and `header=True` (to include the column headers in the first row of the new file).\n",
        "4. **Confirmation Message:** After saving, it prints a message indicating whether rows were appended to an existing file or a new file was created, along with the number of rows processed."
      ],
      "metadata": {
        "id": "rXiLPazsP-f3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a753f55b"
      },
      "source": [
        "def save_dataframe_to_csv(df, path):\n",
        "    \"\"\"\n",
        "    Saves a DataFrame to a CSV file. If the file exists, it appends the data without headers.\n",
        "    Otherwise, it creates a new file with headers.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to save.\n",
        "        path (str): The file path where the CSV should be saved.\n",
        "    \"\"\"\n",
        "    if os.path.exists(path):\n",
        "        df.to_csv(path, mode='a', header=False, index=False)\n",
        "        print(f\"Appended {len(df)} rows to existing file: {path}\")\n",
        "    else:\n",
        "        df.to_csv(path, mode='w', header=True, index=False)\n",
        "        print(f\"Created new file and wrote {len(df)} rows: {path}\")\n",
        "\n",
        "print(\"Defined save_dataframe_to_csv_smart function.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meetings\n",
        "First get the complete data set with all **Meetings** where each meeting is either a testing or racing event covering multiple sessions and days.\\\n",
        "Save the data to a CSV file replacing any previous file."
      ],
      "metadata": {
        "id": "99LFxEbtqsK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meetings = get_data('meetings')\n",
        "meetings.to_csv(meetings_path, index=False)"
      ],
      "metadata": {
        "id": "_H1ppOuCtVR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qualifying sessions\n",
        "Second, get all of the qualifying sessions. This includes both the sprint qualifying and the qualifying for the feature race. This always gets every session and does not check the existing data storage first.\\\n",
        "Save the data to a CSV file replacing any previous file.\n",
        "\n"
      ],
      "metadata": {
        "id": "LrHNzZs5Lg2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_session_type = \"Qualifying\" # @param [\"Practice\",\"Qualifying\",\"Race\",\"All\"]\n",
        "\n",
        "if selected_session_type == \"All\":\n",
        "    sessions_filter = ''\n",
        "else:\n",
        "    sessions_filter = f'session_type={selected_session_type}'\n",
        "\n",
        "sessions_filter = sessions_filter.replace(' ', '')   # Remove whitespace\n",
        "\n",
        "sessions = get_data(endpoint = 'sessions',\n",
        "                    filter = sessions_filter)\n",
        "\n",
        "# Convert the date columns to datetime objects for comparison\n",
        "sessions['date_start'] = pd.to_datetime(sessions['date_start'])\n",
        "sessions['date_end'] = pd.to_datetime(sessions['date_end'])\n",
        "# Remove all session where the end time is after the current time\n",
        "sessions = sessions[sessions['date_end'] <= pd.Timestamp.now(tz='UTC')]\n",
        "\n",
        "sessions.to_csv(sessions_path, index=False)\n",
        "\n",
        "# Create a list of all the unique sessions\n",
        "# The session_key should be unique in the dataframe to begin with\n",
        "all_session_keys = sessions['session_key'].unique()\n",
        "print(f\"Total number of sessions: {len(all_session_keys)}\")"
      ],
      "metadata": {
        "id": "kw9jUxW7t65C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drivers\n",
        "\n",
        "Retrieve the driver information for each session and store it in a local file.\n",
        "- First check for an existing CSV file and the sessions that are already stored in the file.\n",
        "- Get the driver information from the API but only from any qualifying sessions that have not been retrieved earlier.\n",
        "- Then append the newly retrieved data to the existing CSV file.\n",
        "\n",
        "*This method is slower than just getting the full data set with all drivers from all sessions in a request.\\\n",
        "However, as the number of sessions increase, that might come up against the size limit for each individual response from the API.*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lVYOcqLS1Yvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the file already exists, read it and\n",
        "# identify the sessions that have not yet been retrieved\n",
        "\n",
        "drivers_csv_file_exists = os.path.exists(drivers_path)\n",
        "\n",
        "if not drivers_csv_file_exists:\n",
        "    # Create an empty dataframe\n",
        "    drivers_from_csv = []\n",
        "    retrieved_session_keys = empty(0)\n",
        "else:\n",
        "    # Load the data from the file\n",
        "    drivers_from_csv = pd.read_csv(drivers_path)\n",
        "    # Get a list of unique sessions\n",
        "    retrieved_session_keys = drivers_from_csv['session_key'].unique()\n",
        "\n",
        "# Identify the sessions that do not appear in the 'Drivers' data file\n",
        "# The drivers from missing sessions will need to be retrived\n",
        "unretrieved_session_keys = list(set(all_session_keys) - set(retrieved_session_keys))\n",
        "\n",
        "print(f\"Number of sessions already retrieved: {len(retrieved_session_keys)}\")\n",
        "print(f\"Number of sessions to be retrieved: {len(unretrieved_session_keys)}\")"
      ],
      "metadata": {
        "id": "Oc_rJTF-2vOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drivers = []\n",
        "\n",
        "# Get the laps from every session not already retrieved\n",
        "for session_key in unretrieved_session_keys:\n",
        "    drivers_by_session = get_data('drivers', f'session_key={session_key}')\n",
        "    drivers.append(drivers_by_session)\n",
        "\n",
        "print(f\"Fetched driver data for {len(drivers)} sessions.\")"
      ],
      "metadata": {
        "id": "MvTRUMMs37g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if drivers:\n",
        "    # The laps dataframe is a dataframe of dataframes\n",
        "    # This collapses the dataframes\n",
        "    drivers = pd.concat(drivers, ignore_index=True)\n",
        "    save_dataframe_to_csv(df=drivers, path=drivers_path)\n",
        "\n",
        "else:\n",
        "    print(\"No new lap data to append or save.\")\n",
        "    laps = pd.DataFrame() # Ensure laps is always a DataFrame, even if empty"
      ],
      "metadata": {
        "id": "tr4cHDVh9kUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laps\n",
        "Retrieve the Lap data and store it in a local file before identifying the fastest qualifying laps for each driver in each session.\n",
        "- First check for an existing CSV file and the laps that are already stored in the file.\n",
        "- Get the lap data from the API but only from any qualifying sessions that have not been retrieved earlier.\n",
        "- Then append the newly retrieved data to the existing CSV file.\n",
        "\n"
      ],
      "metadata": {
        "id": "WoQVou1w0sqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the file already exists, read it and\n",
        "# identify the sessions that have not yet been retrieved\n",
        "\n",
        "laps_csv_file_exists = os.path.exists(laps_path)\n",
        "\n",
        "if not laps_csv_file_exists:\n",
        "    # Create an empty dataframe\n",
        "    laps_from_csv = []\n",
        "    retrieved_session_keys = empty(0)\n",
        "else:\n",
        "    # Load the data from the file\n",
        "    laps_from_csv = pd.read_csv(laps_path)\n",
        "    # Get a list of unique sessions\n",
        "    retrieved_session_keys = laps_from_csv['session_key'].unique()\n",
        "\n",
        "# Identify the sessions that do not appear in the 'Laps' data file\n",
        "# The laps from missing sessions will need to be retrived\n",
        "unretrieved_session_keys = list(set(all_session_keys) - set(retrieved_session_keys))\n",
        "\n",
        "print(f\"Number of sessions already retrieved: {len(retrieved_session_keys)}\")\n",
        "print(f\"Number of sessions to be retrieved: {len(unretrieved_session_keys)}\")\n",
        "# The lap data from the qualifying in Baku in 2025 is missing from OpenF1.org\n",
        "# and the program will try to pick it up every time it is executed"
      ],
      "metadata": {
        "id": "wV57ac6VL9Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a7c5fc6"
      },
      "source": [
        "laps = []\n",
        "\n",
        "# Get the laps from every session not already retrieved\n",
        "for session_key in unretrieved_session_keys:\n",
        "    laps_by_session = get_data('laps', f'session_key={session_key}')\n",
        "    laps.append(laps_by_session)\n",
        "\n",
        "print(f\"Fetched lap data for {len(laps)} sessions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43384980"
      },
      "source": [
        "if laps:\n",
        "    # The laps dataframe is a dataframe of dataframes\n",
        "    # This collapses the dataframes\n",
        "    laps = pd.concat(laps, ignore_index=True)\n",
        "    save_dataframe_to_csv(laps, laps_path)\n",
        "\n",
        "else:\n",
        "    print(\"No new lap data to append or save.\")\n",
        "    laps = pd.DataFrame() # Ensure laps is always a DataFrame, even if empty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46366ade"
      },
      "source": [
        "# Identify the fastest lap from each driver in every session\n",
        "\n",
        "To find the fastest lap for each driver in each session, we need to:\n",
        "1. Ensure the `laps` DataFrame contains data.\\\n",
        "If `laps` is currently a list of DataFrames, we will concatenate it into a single DataFrame.\n",
        "2. Group the DataFrame by `session_key` and `driver_number`.\n",
        "3. For each group, find the row with the minimum `lap_duration`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5da20df3"
      },
      "source": [
        "if not laps.empty:\n",
        "    # Create a copy to avoid SettingWithCopyWarning and resets the index\n",
        "    fastest_laps = laps.copy().reset_index(drop=True)\n",
        "\n",
        "    # Convert is_pit_out_lap to boolean and drop rows where is_pit_out_lap is True\n",
        "    fastest_laps['is_pit_out_lap'] = fastest_laps['is_pit_out_lap'].astype(bool)\n",
        "    fastest_laps = fastest_laps[fastest_laps['is_pit_out_lap'] == False]\n",
        "    # Ensure 'date_start' is datetime and 'lap_duration' is numeric\n",
        "    fastest_laps['date_start'] = pd.to_datetime(fastest_laps['date_start'],\n",
        "                                                format='ISO8601')\n",
        "    fastest_laps['lap_duration'] = pd.to_numeric(fastest_laps['lap_duration'],\n",
        "                                                 errors='coerce')\n",
        "    # Drop rows where lap_duration is NaN (couldn't be converted)\n",
        "    fastest_laps.dropna(subset=['lap_duration'], inplace=True)\n",
        "\n",
        "    # Calculate 'date_end' by adding 'lap_duration' to 'date_start'\n",
        "    fastest_laps['date_end'] = fastest_laps['date_start'] \\\n",
        "                               + pd.to_timedelta(fastest_laps['lap_duration'], unit='s')\n",
        "\n",
        "    # Find the fastest lap for each driver in each session\n",
        "    fastest_laps = fastest_laps.loc[fastest_laps.groupby(['session_key', 'driver_number'])['lap_duration'].idxmin()]\n",
        "\n",
        "    print(f\"Found {len(fastest_laps)} fastest laps.\")\n",
        "\n",
        "    save_dataframe_to_csv(fastest_laps, fastest_laps_path)\n",
        "\n",
        "else:\n",
        "    fastest_laps = pd.DataFrame() # Ensure fastest_laps is always a DataFrame, even if empty\n",
        "    print(\"No lap data available to process.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get a complete list of all qualifying laps\n",
        "\n",
        "The complete list of qualifying laps will be used to get the car and location data for just those laps.\n",
        "\n",
        "1. Combine the data from the CSV file with any newly\n",
        "retrieved qualifying laps\n",
        "2. Get evey unique combination of the session and the driver\n",
        "3. Merge in the start and end time for each of the fastest qualifying laps\n",
        "\n"
      ],
      "metadata": {
        "id": "Xf69_Zcr2089"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f35bb42e"
      },
      "source": [
        "# Combine the laps from the CSV file with the newly retrieved laps\n",
        "# to get a complete list\n",
        "all_qualifying_laps = pd.concat([laps_from_csv, laps], ignore_index=True)\n",
        "\n",
        "# Create a list of all session and driver combinations\n",
        "unique_laps = all_qualifying_laps[['session_key','driver_number']].drop_duplicates()\n",
        "\n",
        "if not unique_laps.empty and not fastest_laps.empty:\n",
        "    # Select only the necessary columns from fastest_laps to merge\n",
        "    fastest_laps_for_merge = fastest_laps[['session_key', 'driver_number', 'date_start', 'date_end']]\n",
        "\n",
        "    # Merge unique_laps_keys with these selected columns\n",
        "    unique_laps = pd.merge(\n",
        "        unique_laps,\n",
        "        fastest_laps_for_merge,\n",
        "        on=['session_key', 'driver_number'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    print(\"Added 'date_start' and 'date_end' to unique_laps_keys.\")\n",
        "    display(unique_laps.head())\n",
        "else:\n",
        "    print(\"Either unique_laps or fastest_laps DataFrame is empty, cannot add dates.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Car data\n",
        "\n",
        "First check for an existing CSV file and the car data that are already stored in the file.\n",
        "\n",
        "Get the car data for each of the fastst qualifying laps from the API but only from any session and driver combination that have not been retrieved earlier.\n",
        "\n",
        "Then append the newly retrieved data to the existing CSV file."
      ],
      "metadata": {
        "id": "Bae7Hp0D38Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the qualifying laps for which the\n",
        "# car telemetry has not yet been retrieved\n",
        "\n",
        "car_data_csv_file_exists = os.path.exists(car_data_path)\n",
        "\n",
        "if not car_data_csv_file_exists:\n",
        "    # Create an empty dataframe with the expected columns\n",
        "    car_data_from_csv = pd.DataFrame(columns=['session_key', 'driver_number'])\n",
        "    retrieved_car_data_laps = pd.DataFrame(columns=['session_key', 'driver_number'])\n",
        "else:\n",
        "    # Load the data from the file\n",
        "    car_data_from_csv = pd.read_csv(car_data_path)\n",
        "    # Get a list of unique session_key and driver_number combinations that have car data\n",
        "    retrieved_car_data_laps = car_data_from_csv[['session_key','driver_number']].drop_duplicates()\n",
        "\n",
        "# Convert DataFrames to sets of tuples for efficient comparison\n",
        "# This creates a unique identifier for each session-driver combination (session_key, driver_number)\n",
        "set_unique_laps = set(tuple(row) for row in unique_laps[['session_key', 'driver_number']].values)\n",
        "set_retrieved_car_data_laps = set(tuple(row) for row in retrieved_car_data_laps.values)\n",
        "\n",
        "# Identify the unique session_key and driver_number combinations that have not been retrieved\n",
        "unretrieved_combinations_set = set_unique_laps - set_retrieved_car_data_laps\n",
        "\n",
        "# Convert the set of unretrieved combinations back into a DataFrame\n",
        "unretrieved_car_data_laps = pd.DataFrame(list(unretrieved_combinations_set), columns=['session_key', 'driver_number'])\n",
        "\n",
        "\n",
        "if not unretrieved_car_data_laps.empty and not fastest_laps.empty:\n",
        "    # Now, merge with the unique_laps DataFrame to get the date_start and\n",
        "    # date_end for these unretrieved combinations\n",
        "    unretrieved_car_data_laps = pd.merge(\n",
        "        unretrieved_car_data_laps,\n",
        "        unique_laps[['session_key', 'driver_number', 'date_start', 'date_end']],\n",
        "        on=['session_key', 'driver_number'],\n",
        "        how='left'\n",
        "    )\n",
        "    # Remove records that do not have both a date_start and a date_end\n",
        "    unretrieved_car_data_laps.dropna(subset=['date_start', 'date_end'], inplace=True)\n",
        "else:\n",
        "    unretrieved_car_data_laps = pd.DataFrame(columns=['session_key', 'driver_number', 'date_start', 'date_end'])\n",
        "\n",
        "print(f\"Number of laps already retrieved: {len(retrieved_car_data_laps)}\")\n",
        "print(f\"Number of laps to be retrieved: {len(unretrieved_car_data_laps)}\")"
      ],
      "metadata": {
        "id": "E3TiKIjcrLnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98506594"
      },
      "source": [
        "# Request the car telemetry for each of the remaining laps\n",
        "\n",
        "car_data = []\n",
        "\n",
        "# Iterate through each of the missing laps\n",
        "for index, row in unretrieved_car_data_laps.iterrows():\n",
        "\n",
        "    # Get the session and driver number from the current row\n",
        "    session_key = row['session_key']\n",
        "    driver_number = row['driver_number']\n",
        "\n",
        "    # date_start and date_end columns are already datetime objects from previous steps\n",
        "    # We need to ensure they are UTC and then format them.\n",
        "    date_start_utc = row['date_start'].tz_convert('UTC')\n",
        "    date_end_utc = row['date_end'].tz_convert('UTC')\n",
        "\n",
        "    formatted_date_start = date_start_utc.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "    formatted_date_end = date_end_utc.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "    data_filter = f\"session_key={session_key}& \\\n",
        "                    driver_number={driver_number}& \\\n",
        "                    date>={formatted_date_start}& \\\n",
        "                    date<={formatted_date_end}\"\n",
        "    data_filter = data_filter.replace(' ', '')\n",
        "\n",
        "    # Fetching 'car_data'\n",
        "    car_data_by_session_driver = get_data('car_data', data_filter)\n",
        "    if not car_data_by_session_driver.empty:\n",
        "        car_data.append(car_data_by_session_driver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the newly retrieved car telemtry to a file\n",
        "\n",
        "if car_data:\n",
        "    # The 'car_data' dataframe is a dataframe of dataframes\n",
        "    # This collapses the dataframes\n",
        "    car_data = pd.concat(car_data, ignore_index=True)\n",
        "    save_dataframe_to_csv(df=car_data, path=car_data_path)\n",
        "\n",
        "else:\n",
        "    print(\"No new car data to append or save.\")"
      ],
      "metadata": {
        "id": "mUjvlH7YixcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Location\n",
        "\n",
        "First check for an existing CSV file and the location data that are already stored in the file.\n",
        "\n",
        "Get the location data for each of the fastst qualifying laps from the API but only from any session and driver combination that have not been retrieved earlier.\n",
        "\n",
        "Then append the newly retrieved data to the existing CSV file."
      ],
      "metadata": {
        "id": "WdIcrcaw5QjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the qualifying laps for which the\n",
        "# car location has not yet been retrieved\n",
        "\n",
        "location_csv_file_exists = os.path.exists(car_data_path)\n",
        "\n",
        "if not location_csv_file_exists:\n",
        "    # Create an empty dataframe with the expected columns\n",
        "    location_from_csv = pd.DataFrame(columns=['session_key', 'driver_number'])\n",
        "    retrieved_location_laps = pd.DataFrame(columns=['session_key', 'driver_number'])\n",
        "else:\n",
        "    # Load the data from the file\n",
        "    location_from_csv = pd.read_csv(location_path)\n",
        "    # Get a list of unique session_key and driver_number combinations that have car data\n",
        "    retrieved_location_laps = location_from_csv[['session_key','driver_number']].drop_duplicates()\n",
        "\n",
        "# Convert DataFrames to sets of tuples for efficient comparison\n",
        "# This creates a unique identifier for each session-driver combination (session_key, driver_number)\n",
        "set_unique_laps = set(tuple(row) for row in unique_laps[['session_key', 'driver_number']].values)\n",
        "set_retrieved_location_laps = set(tuple(row) for row in retrieved_location_laps.values)\n",
        "\n",
        "# Identify the unique session_key and driver_number combinations that have not been retrieved\n",
        "unretrieved_combinations_set = set_unique_laps - set_retrieved_location_laps\n",
        "\n",
        "# Convert the set of unretrieved combinations back into a DataFrame\n",
        "unretrieved_location_laps = pd.DataFrame(list(unretrieved_combinations_set), columns=['session_key', 'driver_number'])\n",
        "\n",
        "if not unretrieved_location_laps.empty and not fastest_laps.empty:\n",
        "    # Now, merge with the unique_laps DataFrame to get the date_start and\n",
        "    # date_end for these unretrieved combinations\n",
        "    unretrieved_location_laps = pd.merge(\n",
        "        unretrieved_location_laps,\n",
        "        unique_laps[['session_key', 'driver_number', 'date_start', 'date_end']],\n",
        "        on=['session_key', 'driver_number'],\n",
        "        how='left'\n",
        "    )\n",
        "    # Remove records that do not have both a date_start and a date_end\n",
        "    unretrieved_location_laps.dropna(subset=['date_start', 'date_end'], inplace=True)\n",
        "else:\n",
        "    unretrieved_location_laps = pd.DataFrame(columns=['session_key', 'driver_number', 'date_start', 'date_end'])\n",
        "\n",
        "print(f\"Number of laps already retrieved: {len(retrieved_location_laps)}\")\n",
        "print(f\"Number of laps to be retrieved: {len(unretrieved_location_laps)}\")"
      ],
      "metadata": {
        "id": "DNdrUqzD47Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN9xiL8C49Ks"
      },
      "source": [
        "# Request the car location for each of the remaining laps\n",
        "\n",
        "location = []\n",
        "\n",
        "# Iterate through each of the missing laps\n",
        "for index, row in unretrieved_location_laps.iterrows():\n",
        "    # Get the session and driver number from the current row\n",
        "    session_key = row['session_key']\n",
        "    driver_number = row['driver_number']\n",
        "\n",
        "    # date_start and date_end columns are already datetime objects from previous steps\n",
        "    # We need to ensure they are UTC and then format them.\n",
        "    date_start_utc = row['date_start'].tz_convert('UTC')\n",
        "    date_end_utc = row['date_end'].tz_convert('UTC')\n",
        "\n",
        "    formatted_date_start = date_start_utc.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "    formatted_date_end = date_end_utc.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
        "\n",
        "    data_filter = f\"session_key={session_key}& \\\n",
        "                    driver_number={driver_number}& \\\n",
        "                    date>={formatted_date_start}& \\\n",
        "                    date<={formatted_date_end}\"\n",
        "    data_filter = data_filter.replace(' ', '')\n",
        "\n",
        "    # Fetching 'location'\n",
        "    location_by_session_driver = get_data('location', data_filter)\n",
        "    if not location_by_session_driver.empty:\n",
        "        location.append(location_by_session_driver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the newly retrieved car telemtry to a file\n",
        "\n",
        "if location:\n",
        "    # The 'location' dataframe is a dataframe of dataframes\n",
        "    # This collapses the dataframes\n",
        "    location = pd.concat(location, ignore_index=True)\n",
        "    save_dataframe_to_csv(df=location, path=location_path)\n",
        "\n",
        "else:\n",
        "    print(\"No new location data to append or save.\")"
      ],
      "metadata": {
        "id": "8wVO-EpVj6Da"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}